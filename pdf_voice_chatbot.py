# -*- coding: utf-8 -*-
"""pdf_voice_chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wuGgJBB0_mEb5u7UD5_1o4TDZDciXr1P
"""

import numpy as np
import pandas as pd

!pip install langchain
!pip install cohere
!pip install PyPDF2
!pip install faiss-cpu
!pip install gradio

!pip install -U langchain-community

import openai

from PyPDF2 import PdfReader
from langchain_community.embeddings import CohereEmbeddings
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS

import os
os.environ["COHERE_API_KEY"] = "sWpCEwZV6D6nSi71sK2aPB4a84Q1kkuSxT5gQ8ri"
pdfreader = PdfReader('/content/file-example_PDF_500_kB.pdf')

"""
**Extracting Text from PDF and splittering into smaller chunks**"""

from typing_extensions import Concatenate
# read text from pdf
raw_text = ''
for i, page in enumerate(pdfreader.pages):
    content = page.extract_text()
    if content:
        raw_text += content

# We need to split the text using Character Text Split such that it sshould not increse token size
text_splitter = CharacterTextSplitter(
    separator = "\n",
    chunk_size = 800,
    chunk_overlap  = 200,
    length_function = len,
)
texts = text_splitter.split_text(raw_text)

"""**Downloading embeddings to index and atttaching FAISS vector store for future searching**"""

from langchain.embeddings import HuggingFaceEmbeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

document_search = FAISS.from_texts(texts, embeddings)

"""Retrival and Generating Components of RAG"""

from langchain.chains.question_answering import load_qa_chain
from langchain.llms import Cohere
chain = load_qa_chain(Cohere(), chain_type="stuff")

import PyPDF2
def load_and_chunk_pdf(pdf_path, chunk_size=300):
    with open(pdf_path, 'rb') as file:
        reader = PyPDF2.PdfReader(file)
        text = " ".join(page.extract_text() for page in reader.pages if page.extract_text())
    chunks = [text[i:i+chunk_size] for i in range(0, len(text), chunk_size)]
    return chunks

pdf_chunks = load_and_chunk_pdf("/content/file-example_PDF_500_kB.pdf")

from sentence_transformers import SentenceTransformer
embedder = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
chunk_embeddings = embedder.encode(pdf_chunks)

import faiss
dimension = chunk_embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(chunk_embeddings))

"""**Speech-to-Text using Faster-Whisper**"""

!pip install faster_whisper

from faster_whisper import WhisperModel
whisper_model = WhisperModel("base", compute_type="int8")

def transcribe_audio(audio_file):
    segments, _ = whisper_model.transcribe(audio_file, beam_size=5)
    return " ".join([segment.text for segment in segments])

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

llm_model = "tiiuae/falcon-rw-1b"  # changeable to LLaMA Scout if available
tokenizer = AutoTokenizer.from_pretrained(llm_model)
model = AutoModelForCausalLM.from_pretrained(llm_model, device_map="auto")
llm_pipe = pipeline("text-generation", model=model, tokenizer=tokenizer)

def answer_query(user_question):
    # Embed question
    question_embedding = embedder.encode([user_question])
    _, top_k_indices = index.search(question_embedding, k=3)
    retrieved_context = "\n".join([pdf_chunks[i] for i in top_k_indices[0]])

    prompt = f"""Answer the following based on context:\n\nContext:\n{retrieved_context}\n\nQuestion: {user_question}"""
    response = llm_pipe(prompt, max_new_tokens=150, do_sample=True, temperature=0.7)
    return response[0]["generated_text"].split("Question:")[-1].strip()

"""**Text-to-Speech using HuggingFace TTS (Gemini or Bark)**"""

from transformers import BarkModel, BarkProcessor
import torch
import scipy.io.wavfile

processor = BarkProcessor.from_pretrained("suno/bark")
bark_model = BarkModel.from_pretrained("suno/bark").to("cpu")

def speak_answer(text):
    inputs = processor(text, return_tensors="pt").to("cpu")
    audio_output = bark_model.generate(**inputs)
    scipy.io.wavfile.write("output.wav", rate=22050, data=audio_output.cpu().numpy())
    return "output.wav"

"""**Gradio App: Voice → Query → LLM → Speech Output**"""

!pip install -U gradio

import gradio as gr

import traceback

def chatbot_interface(audio_input):
    try:
        print("Received audio file path:", audio_input)

        if not audio_input or not os.path.exists(audio_input):
            return "No valid audio file found", None

        text = transcribe_audio(audio_input)
        print("Transcribed text:", text)

        response = answer_query(text)
        print("LLM Response:", response)

        audio_path = speak_answer(response)
        print("Generated audio at:", audio_path)

        return response, audio_path

    except Exception as e:
        print("ERROR:", str(e))
        traceback.print_exc()  # full traceback
        return "Error occurred during processing.", None

gr.Interface(
    fn=chatbot_interface,
    inputs=gr.Audio(sources=["microphone"], type="filepath", label="Speak your query"),
    outputs=[gr.Text(label="Answer"), gr.Audio(label="Voice Output")],
    title="Voice-Enabled Chat with PDFs",
    description="Ask questions from a PDF using voice input. Answer is generated using LLM and read out loud using TTS."
).launch()

